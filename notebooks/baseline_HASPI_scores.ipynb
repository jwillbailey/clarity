{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_HASPI_scores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPq+l8KP6dJKJklLJdseepL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwillbailey/clarity/blob/main/notebooks/baseline_HASPI_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generating Baseline HASPI Scores**\n",
        "\n",
        "The Clarity Enhancement Challenge 2 (CEC2) comes with a baseline enhancement model for participants to benchmark and compare their systems performance. <br><br>\n",
        "\n",
        "To access the enhancer and sample data, first clone the Github repository and run the functions to get the metadata and scenes components of the demo dataset:  "
      ],
      "metadata": {
        "id": "4DPiB7WXx2hf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_AtYPIuxwDD"
      },
      "outputs": [],
      "source": [
        "print('Cloning git repo...')\n",
        "!git clone --quiet https://github.com/jwillbailey/clarity.git\n",
        "\n",
        "print('Changing directory...')\n",
        "%cd clarity\n",
        "\n",
        "print('Installing requirements with pip...')\n",
        "!pip install -qr requirements.txt\n",
        "!more setup.py\n",
        "print('Setting up toolkit modules...')\n",
        "!pip install -e .\n",
        "%cd /content/\n",
        "\n",
        "import clarity\n",
        "from clarity.notebooks import demo_data\n",
        "\n",
        "demo_data.get_metadata_demo()\n",
        "demo_data.get_scenes_demo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The baseline enhancer is based on <a href='https://pubmed.ncbi.nlm.nih.gov/3743918/'>NAL-R prescription fitting</a>. Since output signals are required to be in 16-bit integer format, a slow acting automatic gain control is implemented to reduce clipping of the signal introduced by the NAL-R fitting for audiograms which represent more severe hearing loss.\n",
        "\n",
        "The NAL-R and AGC (compressor) classes can be accessed by importing them from the <code>clarity.enhancer</code> module."
      ],
      "metadata": {
        "id": "5px0mOH8x1Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clarity.clarity.enhancer.nalr import NALR\n",
        "from clarity.clarity.enhancer.compressor import Compressor\n"
      ],
      "metadata": {
        "id": "QhxhEF0Uvaj8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "NAL-R fitting involves creating a complimentary filterbank based on the audiogram of a listener. Accessing listener data from scene definitions is covered in more detail in <a href='https://github.com/jwillbailey/clarity/blob/main/notebooks/Installing_clarity_tools_and_using_metadata.ipynb'>this notebook</a>\n",
        "\n",
        "Listener data should be loaded from a scene definition from the metadata set.\n",
        "<br><br>\n",
        "Firstly, load in the scene, listeners and scene_listeners metadata files:"
      ],
      "metadata": {
        "id": "9PjbHgS-yJW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('demo/metadata/scenes.demo.json') as f:\n",
        "  scene_metadata = json.load(f)\n",
        "\n",
        "with open('demo/metadata/listeners.json') as f:\n",
        "  listeners_metadata = json.load(f)\n",
        "\n",
        "with open('demo/metadata/scenes_listeners.dev.json') as f:\n",
        "  scene_listeners_metadata = json.load(f)"
      ],
      "metadata": {
        "id": "gXySwG8UyIOl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, index a scene from <code>scenes_metadata</code> and use it to retrieve an audiogram from the <code>listeners_metadata</code> via the scene ID entry in <code>scene_listeners_metadata</code>. In practice, this procedure is performed within a loop.\n",
        "\n",
        "Each scene has three listeners which have been randomly selected for the given scene. Accessing via the scene ID ensures the correct listener is accessed and will be consistent with the evaluation procedure of the challenge.\n",
        "\n",
        "Each listener metadata entry is a dict containing:\n",
        "\n",
        "- Name\n",
        "- Audiogram centre frequencies\n",
        "- Left ear audiogram hearing levels (dBHL)\n",
        "- Right ear audiogram hearing levels (dBHL)\n"
      ],
      "metadata": {
        "id": "d0X8JXOr0grB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scene_index = 0\n",
        "listener_choice = 0\n",
        "\n",
        "scene = scene_metadata[scene_index]\n",
        "\n",
        "scene_id = scene['scene']\n",
        "\n",
        "scene_listeners = scene_listeners_metadata[scene_id]\n",
        "\n",
        "listener = listeners_metadata[scene_listeners[listener_choice]]\n",
        "\n",
        "print(listener)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejK7ulc41Oa2",
        "outputId": "2bd319d0-743e-40ed-bb09-0af483fb9188"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'L0064', 'audiogram_cfs': [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], 'audiogram_levels_l': [40, 30, 20, 50, 60, 65, 80, 75], 'audiogram_levels_r': [40, 35, 30, 50, 60, 75, 80, 80]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "To allow for scalable and flexible running on both local and HPC platforms, many clarity challenge CEC2 scripts and tools depend on <a href='https://hydra.cc/'>hydra</a> and <a href='https://github.com/facebookincubator/submitit'>submitit</a> for the configuration of python code, for the setting of environment variables such as dataset directories, and for enabling parallisation of python on both HPC and local machines. A full description of how hydra and submitit is used in the clarity challenges is out of the scope of this notebook, but more details can be found <a href=''>here</a>.\n",
        "\n",
        "For the sake of this notebook, we will be importing a configuration file directly using <code>omegaconf</code>."
      ],
      "metadata": {
        "id": "RAMaIjLE36l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "cfg = OmegaConf.load('clarity/recipes/cec2/baseline/config.yaml')\n",
        "assert type(cfg)==DictConfig"
      ],
      "metadata": {
        "id": "FDEhmbS86g3s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ordinary use cases, parameter overrides are performed by passing arguments in the command line to a <code>$python my_script.py</code> command. However,for the sake of this notebook we will configure the <code>conf</code> object directly.\n",
        "\n",
        "We need to supply:\n",
        "- The root directory of the project data and metadata\n",
        "- The directory of the metadata\n",
        "- The directory of the audio data\n",
        "\n",
        "as these will differ from the standard installation paths for the project in this case."
      ],
      "metadata": {
        "id": "7rrfQE_Y-kXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.path['root'] = 'demo'\n",
        "cfg.path['metadata_dir'] = '${path.root}/metadata'\n",
        "cfg.path['scenes_folder'] = '${path.root}/scenes'"
      ],
      "metadata": {
        "id": "WC2pga2Z_kKt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the configuration modified, we can now instantiate our <code>NALR</code> and <code>Compressor</code> objects."
      ],
      "metadata": {
        "id": "LQmWPOQhCCj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enhancer = NALR(**cfg.nalr)\n",
        "compressor = Compressor(**cfg.compressor)\n"
      ],
      "metadata": {
        "id": "koMNeEy2CdVX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next it is necessary to load in audio to process. \n",
        "\n",
        "As with the listener metadata, scene audio should be loaded using the scene ID from <code>scenes_metadata</code>. \n",
        "\n",
        "Signals are stored as 16-bit integer audio and must be converted to floating point before use. "
      ],
      "metadata": {
        "id": "Vn5gT4VZEExs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "import os\n",
        "\n",
        "\n",
        "fs, signal = wavfile.read(\n",
        "  os.path.join(cfg.path.scenes_folder, f\"{scene_id}_mix_CH1.wav\")\n",
        ")\n",
        "\n",
        "\n",
        "signal = signal / 32768.0\n"
      ],
      "metadata": {
        "id": "YMdjWN7KEnua"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stage of processing the audio is to build the appropriate filterbank based on the audiogram data and apply the frequency dependent amplification.\n",
        "\n",
        "Following this, slow AGC is applied and a clip detection pass is performed. A tanh function is applied to remove high frequency distortion components from cliipped samples and the files are converted back to 16-bit integer format for saving."
      ],
      "metadata": {
        "id": "D9CHEDvTmBrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "nalr_fir, _ = enhancer.build(listener['audiogram_levels_l'], listener['audiogram_cfs'])\n",
        "out_l = enhancer.apply(nalr_fir, signal[:, 0])\n",
        "\n",
        "nalr_fir, _ = enhancer.build(listener['audiogram_levels_r'], listener['audiogram_cfs'])\n",
        "out_r = enhancer.apply(nalr_fir, signal[:, 1])\n",
        "\n",
        "out_l, _, _ = compressor.process(out_l)\n",
        "out_r, _, _ = compressor.process(out_r)\n",
        "            \n",
        "enhanced_audio = np.stack([out_l, out_r], axis=1)\n",
        "filename = f\"{scene}_{listener}_HA-output.wav\"\n",
        "\n",
        "n_clipped = np.sum(np.abs(enhanced_audio) > 1.0)\n",
        "if n_clipped > 0:\n",
        "  print(f\"Writing {filename}: {n_clipped} samples clipped\")\n",
        "if cfg.soft_clip:\n",
        "  enhanced_audio = np.tanh(enhanced_audio)\n",
        "np.clip(enhanced_audio, -1.0, 1.0, out=enhanced_audio)\n",
        "signal_16 = (32768.0 * enhanced_audio).astype(np.int16)\n",
        "\n"
      ],
      "metadata": {
        "id": "w2oVdfWz4OBW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "a section about the haspi scores\n",
        "\n",
        "load audio, haspi function  - needs target audio\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EYLajowXt191"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YmN0MlvruSMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}